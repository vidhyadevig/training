{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "99469af6-e332-47d2-aff0-396d6527bfa1",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "###JOINS\n",
    "\n",
    "- PySpark Join is used to combine two DataFrames and by chaining these you can join multiple DataFrames.\n",
    "- It supports all basic join type operations available in traditional SQL like \n",
    "    - INNER (default) \n",
    "    - LEFT OUTER \n",
    "    - RIGHT OUTER\n",
    "    - FULL OUTER \n",
    "    - LEFT ANTI \n",
    "    - LEFT SEMI \n",
    "    - CROSS \n",
    "    - SELF JOIN\n",
    "- PySpark Joins are wider transformations that involve data shuffling across the network.\n",
    "\n",
    "**INNER**:\n",
    "An Inner join combines two DataFrames based on the key (common column) provided and results in rows where there is a matching found. Rows from both DataFrames are dropped with a non-matching key.\n",
    "\n",
    "**LEFT OUTER:**\n",
    "Leftouter join returns all rows from the left dataset regardless of match found on the right dataset when join expression doesn’t match, it assigns null for that record and drops records from right where match not found.\n",
    "\n",
    "**RIGHT OUTER:**\n",
    "Rightouter join is opposite of left join, here it returns all rows from the right dataset regardless of match found on the left dataset, when join expression doesn’t match, it assigns null for that record and drops records from left where match not found.\n",
    "\n",
    "**FULL OUTER:**\n",
    "It combines the results of both left and right outer joins, ensuring that all records from both DataFrames are included in the resulting DataFrame. It includes all rows from both DataFrames and fills in missing values with nulls where there is no match.\n",
    "\n",
    "**LEFT SEMI:**\n",
    "It returns only the rows from the left DataFrame (the first DataFrame mentioned in the join operation) where there is a match with the right DataFrame (the second DataFrame). It does not include any columns from the right DataFrame in the resulting DataFrame. This join type is useful when you only want to filter rows from the left DataFrame based on whether they have a matching key in the right DataFrame.\n",
    "\n",
    "**LEFT ANTI:**\n",
    "It returns only the rows from the left DataFrame (the first DataFrame mentioned in the join operation) where there is no match with the right DataFrame (the second DataFrame). It excludes any rows from the left DataFrame that have a corresponding key in the right DataFrame. This join type is useful when you want to filter out rows from the left DataFrame that have matching keys in the right DataFrame.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "91308d34-fdb5-412f-8ab6-548992f8e238",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# Employee data (some dept IDs intentionally unmatched)\n",
    "emp = [\n",
    "    (1, \"Aarav\",   -1, \"2019\", \"10\", \"M\", 5500),   # Matches Finance\n",
    "    (2, \"Diya\",     1, \"2015\", \"20\", \"F\", 7200),   # Matches Marketing\n",
    "    (3, \"Karan\",    1, \"2017\", \"10\", \"M\", 4800),   # Matches Finance\n",
    "    (4, \"Isha\",     2, \"2012\", \"30\", \"F\", 6100),   # Matches Sales\n",
    "    (5, \"Rahul\",    2, \"2016\", \"60\", \"M\", 5300),   # No matching dept\n",
    "    (6, \"Neha\",     3, \"2020\", \"70\", \"F\", 4600),   # No matching dept\n",
    "    (7, \"Vikram\",   3, \"2018\", \"30\", \"M\", 5800)    # Matches Sales\n",
    "]\n",
    "\n",
    "empColumns = [\"emp_id\",\"name\",\"superior_emp_id\",\"year_joined\",\"emp_dept_id\",\"gender\",\"salary\"]\n",
    "empDF = spark.createDataFrame(data=emp, schema=empColumns)\n",
    "empDF.show(truncate=False)\n",
    "\n",
    "# Department data (some depts without employees)\n",
    "dept = [\n",
    "    (\"Finance\",   10),\n",
    "    (\"Marketing\", 20),\n",
    "    (\"Sales\",     30),\n",
    "    (\"IT\",        40),    # No employees\n",
    "    (\"HR\",        50)     # No employees\n",
    "]\n",
    "\n",
    "deptColumns = [\"dept_name\",\"dept_id\"]\n",
    "deptDF = spark.createDataFrame(data=dept, schema=deptColumns)\n",
    "deptDF.show(truncate=False)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "d81cf502-648a-4707-b63a-bbe77f19a9e9",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# Inner join\n",
    "empDF.join(deptDF,empDF.emp_dept_id ==  deptDF.dept_id,\"inner\") \\\n",
    "     .show(truncate=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "21c80d98-271b-414f-9345-14fd7eaa3322",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# Left outer join\n",
    "empDF.join(deptDF,empDF.emp_dept_id ==  deptDF.dept_id,\"left\") \\\n",
    "    .show(truncate=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "5532ff40-72b8-4f5d-9def-5ed688f7be8f",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# Right outer join\n",
    "empDF.join(deptDF,empDF.emp_dept_id ==  deptDF.dept_id,\"right\") \\\n",
    "   .show(truncate=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "e0276a02-98ca-440c-b0c6-0f99838e60f9",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# Full outer join\n",
    "empDF.join(deptDF,empDF.emp_dept_id ==  deptDF.dept_id,\"full\") \\\n",
    "    .show(truncate=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "b8003f99-7e1d-489a-8827-c85bb7aedfdc",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# Left semi join\n",
    "empDF.join(deptDF,empDF.emp_dept_id ==  deptDF.dept_id,\"leftsemi\") \\\n",
    "   .show(truncate=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "fe7df1bb-1543-456a-9ec1-e8a6569c9826",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# Left anti join\n",
    "empDF.join(deptDF,empDF.emp_dept_id ==  deptDF.dept_id,\"leftanti\") \\\n",
    "   .show(truncate=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "38169a4d-ad0f-48aa-aec6-272aec68f358",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# Self join\n",
    "\n",
    "from pyspark.sql.functions import col\n",
    "\n",
    "empDF.alias(\"emp1\").join(empDF.alias(\"emp2\"), \\\n",
    "    col(\"emp1.superior_emp_id\") == col(\"emp2.emp_id\"),\"inner\") \\\n",
    "    .select(col(\"emp1.emp_id\"),col(\"emp1.name\"), \\\n",
    "      col(\"emp2.emp_id\").alias(\"superior_emp_id\"), \\\n",
    "      col(\"emp2.name\").alias(\"superior_emp_name\")) \\\n",
    "   .show(truncate=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "f8be6b93-8320-4438-a1db-30c66fd7b07d",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# Cross join\n",
    "empDF.join(deptDF, how=\"cross\") \\\n",
    "     .show(truncate=False)"
   ]
  }
 ],
 "metadata": {
  "application/vnd.databricks.v1+notebook": {
   "computePreferences": {
    "hardware": {
     "accelerator": null,
     "gpuPoolId": null,
     "memory": null
    }
   },
   "dashboards": [],
   "environmentMetadata": {
    "base_environment": "",
    "environment_version": "4"
   },
   "inputWidgetPreferences": null,
   "language": "python",
   "notebookMetadata": {
    "pythonIndentUnit": 4
   },
   "notebookName": "9-joins",
   "widgets": {}
  },
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
