{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "45dd5371-db7e-452a-a8f0-05624914dfa4",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "### PySpark Explode Array and Map Columns to Rows\n",
    "\n",
    "1) \"**explode**(e: Column)\" is used to explode or create array or map columns to rows. \n",
    "\n",
    "      When an array is passed to this function, it creates a new default column “col1” and it contains all array elements. \n",
    "      When a map is passed, it creates two new columns one for key and one for value and each element in map split into the rows.\n",
    "      This will ignore elements that have null or empty.\n",
    "\n",
    "2) \"**explode_outer**(e: Column)\" is used to create a row for each element in the array or map column. \n",
    "      Unlike explode, if the array or map is null or empty, explode_outer returns null.\n",
    "\n",
    "3) \"**posexplode**(e: Column)\" creates a row for each element in the array and creates two columns “pos’ to hold the position of the array element and the ‘col’ to hold the actual array value. And when the input column is a map, posexplode function creates 3 columns “pos” to hold the position of the map element, “key” and “value” columns.This will ignore elements that have null or empty.\n",
    "\n",
    "4) \"**posexplode_outer**(e: Column)\" creates a row for each element in the array and creates two columns “pos’ to hold the position of the array element and the ‘col’ to hold the actual array value. Unlike posexplode, if the array or map is null or empty, posexplode_outer function returns null, null for pos and col columns. Similarly for the map, it returns rows with nulls."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "809ba572-b6b3-4144-b028-6b460a1c887e",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# Create data\n",
    "arrayData = [\n",
    "    ('Aarav', ['Python', 'SQL'], {'hair': 'black', 'eye': 'brown'}),\n",
    "    ('Diya', ['Spark', 'Python', None], {'hair': 'brown', 'eye': None}),\n",
    "    ('Karan', ['Go', ''], {'hair': 'red', 'eye': ''}),\n",
    "    ('Isha', None, None),\n",
    "    ('Rahul', ['1', '2'], {})\n",
    "]\n",
    "\n",
    "# Create DataFrame\n",
    "df = spark.createDataFrame(data=arrayData, schema=['name', 'knownLanguages', 'properties'])\n",
    "\n",
    "# Print schema and show data\n",
    "df.printSchema()\n",
    "df.show(truncate=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "3c93468c-91ca-4cf3-8e27-d445fe47a6d1",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# explode() on array column\n",
    "from pyspark.sql.functions import explode\n",
    "df2 = df.select(df.name,explode(df.knownLanguages))\n",
    "df2.printSchema()\n",
    "df2.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "8290e69d-6556-4c2c-8228-2bcd17d4afa1",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# explode() on map column\n",
    "from pyspark.sql.functions import explode\n",
    "df3 = df.select(df.name,explode(df.properties))\n",
    "df3.printSchema()\n",
    "df3.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "f02cc73d-bd3a-405c-b075-0b7b939e9217",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# explode_outer() on array and map column\n",
    "from pyspark.sql.functions import explode_outer\n",
    "df.select(df.name,explode_outer(df.knownLanguages)).show()\n",
    "df.select(df.name,explode_outer(df.properties)).show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "964b82a7-b052-4ceb-8577-0becd425a0a2",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# posexplode() on array and map\n",
    "from pyspark.sql.functions import posexplode\n",
    "df.select(df.name,posexplode(df.knownLanguages)).show()\n",
    "df.select(df.name,posexplode(df.properties)).show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "c29879eb-5ecb-47fe-b1f1-5ab1d2327352",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# posexplode_outer() on array and map \n",
    "from pyspark.sql.functions import posexplode_outer\n",
    "df.select(\"name\",posexplode_outer(\"knownLanguages\")).show()\n",
    "df.select(df.name,posexplode_outer(df.properties)).show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "233d4d8e-bd7d-4c85-97e7-02beaac013f9",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "IQ:\n",
    "How can I use explode() with multiple columns?\n",
    "\n",
    "You can use explode() on one column at a time. If you need to explode multiple columns simultaneously, you can chain multiple select() statements."
   ]
  }
 ],
 "metadata": {
  "application/vnd.databricks.v1+notebook": {
   "computePreferences": null,
   "dashboards": [],
   "environmentMetadata": {
    "base_environment": "",
    "environment_version": "4"
   },
   "inputWidgetPreferences": null,
   "language": "python",
   "notebookMetadata": {
    "pythonIndentUnit": 4
   },
   "notebookName": "5-explode-rows-to-cols",
   "widgets": {}
  },
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
