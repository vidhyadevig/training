{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "cb18738d-a7eb-4b0d-aac3-3d9d171e6dfe",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "### PySpark Read CSV File into DataFrame\n",
    "- Read Multiple CSV Files\n",
    "- Read all CSV files in a folder using a wildcard \n",
    "- Read all CSV Files from a Directory\n",
    "\n",
    "Syntax: <br>\n",
    "spark.read.**csv**(\"/path/file.csv\") <br>\n",
    "spark.read.**format**(\"csv\").**load**(\"/path/file.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "6ea2489a-e3da-4334-b37b-f5173fbc03bd",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "file_path = \"/Volumes/workspace/training/test_data/csv\"\n",
    "file_name1 = \"employees.csv\"\n",
    "file_name2 = \"employees_updated_dept.csv\"\n",
    "file_name3 = \"employees_updated_salary.csv\"\n",
    "full_path = f\"{file_path}/{file_name1}\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "d99da99e-953c-465c-b1d1-94504f159ea6",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# Read CSV File\n",
    "# df = spark.read.format(\"csv\").load(full_path)\n",
    "# df = spark.read.csv(full_path)\n",
    "# df = spark.read.csv(full_path, header=True)\n",
    "# df = spark.read.csv(full_path, header=True, inferSchema=True)\n",
    "df = spark.read.options(header=True, inferSchema='True',delimiter=',').csv(full_path)\n",
    "# df.count()\n",
    "display(df)\n",
    "df.printSchema()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "0e61d2d2-cd52-4ded-9a55-bf4413bfe840",
     "showTitle": false,
     "tableResultSettingsMap": {
      "0": {
       "dataGridStateBlob": "{\"version\":1,\"tableState\":{\"columnPinning\":{\"left\":[\"#row_number#\"],\"right\":[]},\"columnSizing\":{},\"columnVisibility\":{}},\"settings\":{\"columns\":{}},\"syncTimestamp\":1762174937686}",
       "filterBlob": null,
       "queryPlanFiltersBlob": null,
       "tableResultIndex": 0
      }
     },
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# Read multiple CSV files\n",
    "df = spark.read.csv([f\"{file_path}/{file_name1}\", f\"{file_path}/{file_name2}\", f\"{file_path}/{file_name3}\"],\\\n",
    "        header=True)\n",
    "display(df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "945630f9-bc36-441f-9f38-8a8bf4067c87",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# Read all csv files using a wildcard (if theyâ€™re in the same folder)\n",
    "df = spark.read.csv(f\"{file_path}/employees*.csv\", header=True, inferSchema=True)\n",
    "display(df)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "77a13cb5-a889-4c9b-b69d-116d026e023f",
     "showTitle": false,
     "tableResultSettingsMap": {
      "0": {
       "dataGridStateBlob": "{\"version\":1,\"tableState\":{\"columnPinning\":{\"left\":[\"#row_number#\"],\"right\":[]},\"columnSizing\":{},\"columnVisibility\":{}},\"settings\":{\"columns\":{}},\"syncTimestamp\":1762175829665}",
       "filterBlob": null,
       "queryPlanFiltersBlob": null,
       "tableResultIndex": 0
      }
     },
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    " \n",
    "# Read all files from a directory\n",
    "df = spark.read.csv(file_path, header=True)\n",
    "display(df)\n"
   ]
  }
 ],
 "metadata": {
  "application/vnd.databricks.v1+notebook": {
   "computePreferences": null,
   "dashboards": [],
   "environmentMetadata": {
    "base_environment": "",
    "environment_version": "4"
   },
   "inputWidgetPreferences": null,
   "language": "python",
   "notebookMetadata": {
    "pythonIndentUnit": 4
   },
   "notebookName": "1.1-csv-file-read",
   "widgets": {}
  },
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
