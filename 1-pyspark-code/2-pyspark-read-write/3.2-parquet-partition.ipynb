{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "0793a915-eb24-4a22-b333-ef474b2c384e",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "####Creating a Parquet Partitioned File\n",
    "\n",
    "When we run a query on the Parquet table, Spark scans through all the rows to return the required results â€” similar to how queries work in traditional databases.\n",
    "\n",
    "In PySpark, we can optimize query performance by dividing the data into partitions based on specific columns using the partitionBy() method. Partitioning allows Spark to read only the relevant subsets of data instead of scanning the entire dataset.\n",
    "\n",
    "_Example_:\n",
    "\n",
    "df.write.**partitionBy**(\"gender\", \"salary\").mode(\"overwrite\").parquet(\"/path/file_name.parquet\")\n",
    "\n",
    "\n",
    "In this example, the data is written as a Parquet file and partitioned by gender and salary, which helps improve the efficiency of future queries on these columns."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "6a7fae10-dc8a-4a47-b11f-c4d4e4403b73",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "file_path = \"/Volumes/workspace/training/test_data/parquet\"\n",
    "file_name1 = \"employee_partition.parquet\"\n",
    "full_path = f\"{file_path}/{file_name1}\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "0c60a40a-23b1-4089-a197-4f5f4bd33a05",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# Create dataframe\n",
    "data = [\n",
    "    (\"Aarav\", \"Kumar\", \"Patel\", \"1993-08-14\", \"M\", 5500),\n",
    "    (\"Diya\", \"Rani\", \"Sharma\", \"1998-03-22\", \"F\", 6200),\n",
    "    (\"Karan\", \"\", \"Mehta\", \"1989-11-10\", \"M\", 7200),\n",
    "    (\"Meera\", \"Anand\", \"Nair\", \"1995-07-05\", \"F\", 5800),\n",
    "    (\"Rohan\", \"\", \"Verma\", \"1990-12-30\", \"M\", 5000),\n",
    "    (\"Sneha\", \"L.\", \"Reddy\", \"1996-04-18\", \"F\", 6100),\n",
    "    (\"Vikram\", \"\", \"Singh\", \"1988-09-25\", \"M\", 6800),\n",
    "    (\"Priya\", \"G.\", \"Iyer\", \"1992-01-16\", \"F\", 6400),\n",
    "    (\"Aditya\", \"\", \"Khan\", \"1999-02-28\", \"M\", 4700),\n",
    "    (\"Neha\", \"\", \"Chopra\", \"1997-10-12\", \"F\", 5900)\n",
    "]\n",
    "\n",
    "columns = [\"firstname\", \"middlename\", \"lastname\", \"dob\", \"gender\", \"salary\"]\n",
    "\n",
    "df = spark.createDataFrame(data, columns)\n",
    "df.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "948e9360-d32c-4cc7-b125-3705c75da39f",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# Creating a Parquet Partitioned File\n",
    "df.write.partitionBy(\"gender\",\"salary\").mode(\"overwrite\").parquet(f\"{full_path}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "0d571252-4e26-450c-ae4e-f9a83b78b6cc",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# Read partitioned parquet file using read.parquet()\n",
    "parDF=spark.read.parquet(f\"{full_path}\")\n",
    "display(parDF)"
   ]
  }
 ],
 "metadata": {
  "application/vnd.databricks.v1+notebook": {
   "computePreferences": null,
   "dashboards": [],
   "environmentMetadata": {
    "base_environment": "",
    "environment_version": "4"
   },
   "inputWidgetPreferences": null,
   "language": "python",
   "notebookMetadata": {
    "pythonIndentUnit": 4
   },
   "notebookName": "3.2-parquet-partition",
   "widgets": {}
  },
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
