{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "d1965e8a-4de5-44d4-9f6f-e9cba21dfb0d",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "###Writing PySpark DataFrame to JSON file\n",
    "- Writing DataFrame to JSON file<br>\n",
    "- Options while writing JSON files<br>\n",
    "      _path_: Specifies the path where the JSON files will be saved.<br>\n",
    "      _mode_: Specifies the behavior when writing to an existing directory.<br>\n",
    "      _dateFormat_: Specifies the format for date and timestamp columns.<br>\n",
    "- Saving Mode<br>\n",
    "> Append: Appends the data to the existing data in the target location. If the target location does not exist, it creates a new one.<br>\n",
    "> Overwrite: Overwrites the data in the target location if it already exists. If the target location does not exist, it creates a new one.<br>\n",
    "> Ignore: Ignores the operation and does nothing if the target location already exists. If the target location does not exist, it creates a new one.<br>\n",
    "> Error or ErrorIfExists: Throws an error and fails the operation if the target location already exists. This is the default behavior if no saving mode is specified.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "e57b3e91-d067-4629-82df-398c47f24723",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "file_path = \"/Volumes/workspace/training/test_data/json\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "66e0d6de-8481-4172-a60f-0ffd5bf9dbda",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "\n",
    "from pyspark.sql.types import StructType, StructField, StringType, LongType, DateType\n",
    "from datetime import date\n",
    "\n",
    "# Sample schema\n",
    "schema = StructType([\n",
    "    StructField(\"emp_id\", LongType(), True),\n",
    "    StructField(\"first_name\", StringType(), True),\n",
    "    StructField(\"last_name\", StringType(), True),\n",
    "    StructField(\"hire_date\", DateType(), True)\n",
    "])\n",
    "\n",
    "# Sample data\n",
    "data = [\n",
    "    (1, \"Mark\", \"Jack\", date(2017, 9, 14)),\n",
    "    (2, \"Scott\", \"Sam\", date(2022, 1, 20)),\n",
    "    (3, \"Katherine\", \"Angel\", date(2016, 1, 18))\n",
    "]\n",
    "\n",
    "df = spark.createDataFrame(data, schema)\n",
    "df.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "83144026-5a51-4244-90f9-dab0d506c476",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "\n",
    "df.write \\\n",
    "    .mode(\"overwrite\") \\\n",
    "    .option(\"dateFormat\", \"yyyy-MM-dd\") \\\n",
    "    .json(f\"{file_path}/output\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "75a994c1-4052-4584-99ee-5d381dd1ab6b",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "\n",
    "data = [\n",
    "    (1, \"Mark\", \"Jack\", date(2017, 9, 14)),\n",
    "    (2, \"Scott\", \"Sam\", date(2022, 1, 20)),\n",
    "    (3, \"Kite\", \"\", date(2016, 1, 18))\n",
    "]\n",
    "\n",
    "df = spark.createDataFrame(data, schema)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "847d1202-725e-40b3-8dca-6182f915baa2",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "\n",
    "df.write \\\n",
    "    .mode(\"overwrite\") \\\n",
    "    .option(\"dateFormat\", \"yyyy-MM-dd\") \\\n",
    "    .json(f\"{file_path}/output\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "3cfedbef-a0f4-4d0b-a1de-fbc2656a8ecc",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "new_df = spark.read \\\n",
    "    .option(\"multiLine\", True) \\\n",
    "    .json(f\"{file_path}/output\")\n",
    "\n",
    "display(new_df)"
   ]
  }
 ],
 "metadata": {
  "application/vnd.databricks.v1+notebook": {
   "computePreferences": null,
   "dashboards": [],
   "environmentMetadata": {
    "base_environment": "",
    "environment_version": "4"
   },
   "inputWidgetPreferences": null,
   "language": "python",
   "notebookMetadata": {
    "pythonIndentUnit": 4
   },
   "notebookName": "2.2-json-write",
   "widgets": {}
  },
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
