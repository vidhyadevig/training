{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "b22875a9-ad49-4373-845f-7442f0406f3c",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "###What is Parquet file?\n",
    " Parquet file is a columnar storage format. While querying columnar storage, it skips the non-relevant data very quickly, making query execution faster. As a result, aggregation queries consume less time compared to row-oriented databases. <br>\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "83a797d1-5b89-43e6-848c-fa9936daf254",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "###PySpark Read/Write Parquet File \n",
    "- Read a Parquet file into PySpark Dataframe\n",
    "- Write a PySpark Dataframe to Parquet file \n",
    "\n",
    "Syntax: <br>\n",
    "df.**write**.parquet(\"/path/file.parquet\") <br>\n",
    "df=spark.**read**.parquet(\"/path/file.parquet\") <br>\n",
    "\n",
    "spark.read.**format**(\"json\").**load**(\"/path/file.json\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "d952740a-829c-4ce7-a1bf-4f15d8888e5b",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "file_path = \"/Volumes/workspace/training/test_data/parquet\"\n",
    "file_name1 = \"employees.parquet\"\n",
    "full_path = f\"{file_path}/{file_name1}\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "a699107e-11d4-4d2e-8f70-aeb8f0acf5ad",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# Create dataframe\n",
    "data = [\n",
    "    (\"Aarav\", \"Kumar\", \"Patel\", \"1993-08-14\", \"M\", 5500),\n",
    "    (\"Diya\", \"Rani\", \"Sharma\", \"1998-03-22\", \"F\", 6200),\n",
    "    (\"Karan\", \"\", \"Mehta\", \"1989-11-10\", \"M\", 7200),\n",
    "    (\"Meera\", \"Anand\", \"Nair\", \"1995-07-05\", \"F\", 5800),\n",
    "    (\"Rohan\", \"\", \"Verma\", \"1990-12-30\", \"M\", 5000),\n",
    "    (\"Sneha\", \"L.\", \"Reddy\", \"1996-04-18\", \"F\", 6100),\n",
    "    (\"Vikram\", \"\", \"Singh\", \"1988-09-25\", \"M\", 6800),\n",
    "    (\"Priya\", \"G.\", \"Iyer\", \"1992-01-16\", \"F\", 6400),\n",
    "    (\"Aditya\", \"\", \"Khan\", \"1999-02-28\", \"M\", 4700),\n",
    "    (\"Neha\", \"\", \"Chopra\", \"1997-10-12\", \"F\", 5900)\n",
    "]\n",
    "\n",
    "columns = [\"firstname\", \"middlename\", \"lastname\", \"dob\", \"gender\", \"salary\"]\n",
    "\n",
    "df = spark.createDataFrame(data, columns)\n",
    "df.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "dcf1c6e7-b4e3-40b8-b4cf-23acf8fb8248",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# Write DataFrame to parquet file using write.parquet()\n",
    "df.write.mode(\"overwrite\").parquet(f\"{full_path}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "28b8af0f-ba9e-4958-a9f2-f5512d7be5c9",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# Read parquet file using read.parquet()\n",
    "parDF=spark.read.parquet(f\"{full_path}\")\n",
    "display(parDF)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "c37f26db-fb87-43aa-ae9c-5f4047a0be20",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "####Types of Saving Modes of Parquet File:\n",
    "- df.write.mode(\"**append**\").parquet(\"path/to/parquet/file\")\n",
    "- df.write.mode(\"**overwrite**\").parquet(\"path/to/parquet/file\")\n",
    "- df.write.mode(\"**ignore**\").parquet(\"path/to/parquet/file\")\n",
    "- df.write.mode(\"**error**\").parquet(\"path/to/parquet/file\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "8dbe39cc-6925-4548-8977-fe99d0e89c29",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# Creating a temp view on Parquet file\n",
    "spark.sql(f\"\"\"\n",
    "          CREATE OR REPLACE TEMPORARY VIEW employee \n",
    "          USING parquet \n",
    "          OPTIONS (path '{full_path}')\n",
    "          \"\"\")\n",
    "spark.sql(\"SELECT * FROM employee\").show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "bff29e0a-3d1a-4cbf-a515-d98721c28ce3",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "#  PySpark SQL\n",
    "parDF.createOrReplaceTempView(\"ParquetTable\")\n",
    "spark.sql(\"select * from ParquetTable where salary >= 6000 \").show()"
   ]
  }
 ],
 "metadata": {
  "application/vnd.databricks.v1+notebook": {
   "computePreferences": null,
   "dashboards": [],
   "environmentMetadata": {
    "base_environment": "",
    "environment_version": "4"
   },
   "inputWidgetPreferences": null,
   "language": "python",
   "notebookMetadata": {
    "pythonIndentUnit": 4
   },
   "notebookName": "3.1-parquet-read-write",
   "widgets": {}
  },
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
